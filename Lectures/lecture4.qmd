---
title: '<h1 style="font-size:2.2em; "> Probability and probability distributions<br>& Normal distributions</h1>'
title-slide-attributes:
  data-background-image: ../images/abstract_statistics_playful2.jpeg
  data-background-size: cover
  data-background-opacity: "0.33"
subtitle: "SOC 221 • Lecture 4"
author: "Victoria Sass"
date: "July 1, 2024"
date-format: full
execute: 
  echo: true
  message: false
  warning: false
format: 
  revealjs:
    reference-location: margin
    theme: lecture_styles.scss
    controls: true
    slide-number: true
    chalkboard: true
    incremental: false 
    smaller: true
    preview-links: true
    history: false
    progress: true
    link-external-icon: true
    mermaid-format: svg 
---

```{r}
#| echo: false
#| cache: false
require(tidyverse)
require(gt)

knitr::opts_chunk$set(comment = ">")

# set transparent background in base
knitr::opts_chunk$set(dev.args=list(bg='transparent'))

# set transparent background in ggplot
ggplot2::theme_set(ggplot2::theme_bw(base_size=12))
ggplot2::theme_update(panel.background=ggplot2::element_blank(),
    plot.background=ggplot2::element_blank(),
    legend.background=ggplot2::element_blank())

options(width=80, show.signif.stars=FALSE)
```

# Probability and probability distributions {.section-title background-color="#c5b4e3"}

## Descriptive --> Inferential statistics

:::: {.columns}

::: {.column width="50%"}

::: {.fragment fragment-index=1}
#### Where are we?

* So far: Worked on descriptive statistics – tools to describe distributions
* NEXT: Building the tools for [**inferential statistics**]{.underline}
:::

<br>

::: {.fragment fragment-index=3}
* Example: Want to know about the average study time for the population of UW students.
* NEXT: Building the tools for inferential statistics
:::

:::

::: {.column width="50%"}
::: {.fragment fragment-index=2 style="font-size: 1.4em"}
> [**Inferential statistics**]{style="color:#e93cac"}<br>Statistical procedures used<br>to draw conclusions<br>(or inferences) about<br>a population based on<br>data drawn from a sample
:::

:::

::::

::: {.fragment fragment-index=4 style="position: absolute; bottom: 175px; right: 0px; font-size: 2em"}
$\bar{X}$ ---INFERENCE--> $\mu_x$
:::

## Descriptive --> Inferential statistics

:::: {.columns}

::: {.column width="50%"}

#### Where are we?

* So far: Worked on descriptive statistics – tools to describe distributions
* NEXT: Building the tools for [**inferential statistics**]{.underline}

<br>

* Example: Want to know about the average study time for the population of UW students.
* NEXT: Building the tools for inferential statistics

:::

::: {.column width="50%"}
::: {style="font-size: 1.4em"}
> [**Inferential statistics**]{style="color:#e93cac"}<br>Statistical procedures used<br>to draw conclusions<br>(or inferences) about<br>a population based on<br>data drawn from a sample
:::

:::

::::

::: {style="position: absolute; bottom: 175px; right: 0px; font-size: 2em"}
[$\bar{X}$]{style="color:#1b8883"} ---INFERENCE--> [$\mu_x$]{style="color:#a68100"}
:::

::: {data-id="box4" style="background: #ffc700; border: 4px solid #a68100; border-radius: 15px; width: 350px; height: 150px; padding: 0px 10px 20px 10px; position: absolute; bottom: 0px; right: -75px; font-size: 0.9em"}
Population parameter: The characteristic of the population that we are interested in knowing (i.e. the mean study time of all UW students)
:::

::: {data-id="box2" style="background: #2ad2c9; border: 4px solid #1b8883; border-radius: 15px; width: 375px; height: 125px; padding: 0px 0px 10px 10px; position: absolute; bottom: 20px; right: 325px; font-size: 0.9em"}
Sample statistic: The characteristic of the sample that we actually observe (i.e. the mean study time of a SAMPLE of UW students)
:::

## The challenge of making inferences

::: {.fragment fragment-index=1}
#### Drawing inferences entails uncertainty
Even with a good sample, the sample is likely to differ from the population (sample statistic is likely to be different from our population parameter, just by chance)
:::

<br>

::: {.fragment fragment-index=2}
#### Challenge
Assessing the risk of being wrong (being way off) in making an inference from observed sample statistics to unknown population parameters.
:::

<br>

::: {.fragment fragment-index=3}
#### [Key questions]{style="color: #e93cac;"}
1. How often would this procedure (drawing a sample, inferring about the population) give us something close to the correct answer if I used it over and over?<br>
[**or**]{.r-stack}
2. What is the probability that the inference I draw from one sample is wrong (way off from the population characteristic of interest)?
:::

::: {.fragment fragment-index=4 data-id="box2" style="background: #e93cac; border: 4px solid #690c48; border-radius: 15px; width: 575px; height: 125px; padding: 0px 0px 10px 10px; position: absolute; top: 250px; right: 200px; font-size: 1.5em"}
The answers to these questions rely on concepts of [**probability**]{.underline}
:::



## Probability

::: r-fit-text
> [**Probability**]{style="color:#e93cac"}<br>The **probability** of any outcome of a *random* process is the proportion<br>of times the outcome would occur [in a very long series of repetitions]{.underline}
:::

::: {.fragment fragment-index=1}
$$ 
P = \frac{\text{# of times the outcome of interest could occur in one trial}}{\text{total # of possible outcomes or events}}
$$
:::

:::: {.columns}

::: {.column width="30%"}
::: {.fragment fragment-index=1}
<br>

:::r-fit-text
Probabilities<br>are simply the<br>relative frequency<br>of an outcome
:::
:::
:::

::: {.column width="70%"}
::: {.fragment fragment-index=2}
### [Examples]{style="font-size: 0.75em"}

* Probability of flipping a HEAD with a fair coin:
    * [$P(H) = \frac{1}{2} = .50$]{style="color:#e93cac; font-size: 1.25em;"}

* Probability of rolling a 3 with a 6-sided die
    * [$P(3) = \frac{1}{6} = .1667$]{style="color:#e93cac; font-size: 1.25em;"}
:::
:::

::::

::: {.fragment fragment-index=2 style="position: absolute; bottom: 150px; right: 0px;font-size: 5em;"}
🪙
:::

::: {.fragment fragment-index=2 style="position: absolute; bottom: 0px; right: 0px; font-size: 5em;"}
🎲
::: 

## Random

::: r-fit-text
> [**Random**]{style="color:#e93cac"}<br>We call a phenomenon **random** if individual outcomes<br>are uncertain but there is nonetheless a regular<br>distribution of outcomes in a large number of repetitions.
:::

<br>

::: {.fragment }
### Random = 
::: 

::: incremental

* On any one trial, we don’t know what the outcome will be.
* But there is pattern to the outcomes, so we can guess what will happen if we do something many times (over the long run).
* Random is not the same as haphazard, unpredictable, or unexplained.
* The different possible outcomes of the trial = values of the random variable

:::

## Random

::: r-fit-text
> [**Random**]{style="color:#e93cac"}<br>We call a phenomenon **random** if individual outcomes<br>are uncertain but there is nonetheless a regular<br>distribution of outcomes in a large number of repetitions.
:::

#### [Some random phenomena]{.underline}

::: incremental

* Flipping a coin [(**random variable**: whether result is head or tail)]{style="color:#e93cac"}
* Selecting a single marble from a bag [(**random variable**: color of the marble)]{style="color:#e93cac"}
* Selecting an individual person from a larger group [(**random variable**: level of education (or any other attribute) for the individual)]{style="color:#e93cac"}
* Selecting a random sample from the set of all possible samples [(**random variable**: the value of some statistic (mean, or variance, or correlation, slope, etc.) calculated for the sample)]{style="color:#e93cac"}

:::

## Myths about randomness and probability

### The myth of short-run regularity

:::: {.columns}

::: {.column width="30%"}
::: {.fragment fragment-index=1 style="color:#1b8883"}
[**Myth**]{.underline}: If I flip a coin 10 times I should get 5 heads.
:::

::: {.fragment fragment-index=2 style="color:#6e8e13"}
[**Reality**]{.underline}: Over a short number of flips, you  might get lots of heads or lots of tails.  Probabilities express expectations only over the long run.
:::
:::

::: {.column width="70%"}

::: {.fragment fragment-index=3"}
```{r}
#| echo: false
#| fig-width: 9
#| fig-height: 6
#| fig-align: center

library(gganimate)
library(ggthemes)

set.seed(725)
dice_longrun <- tibble(trial = 1:6000, roll = sample(1:6, size = 6000, replace = TRUE), count_three = NA) 
x <- 0
for(i in 1:6000) {
  if (dice_longrun$roll[i] == 3){
    x = x + 1
  }
  dice_longrun$count_three[i] <- x
}
dice_longrun <- dice_longrun |> mutate(prop_three = count_three/trial)

roll_3 <- ggplot(dice_longrun, aes(x = trial, y = prop_three)) + 
geom_line(color = "#4b2e83") + 
geom_hline(aes(yintercept = 1/6), color = "#e93cac", linetype = 2) + 
annotate(geom = "label", x = 6000, y = 0.2, label = "1/6", color = "#e93cac") + 
labs(title = "Proportion of 3s over 6000 rolls of 6-sided die", x = "Trial #", y = "Proportion of Dice Rolls = 3") +
theme_tufte(base_size = 18) 
# animate(roll_3 + transition_reveal(trial), height = 6, width = 9, units = "in", res = 150)
# anim_save(filename = "lectures/images/dice.gif", animation = last_animation(), duration = 6)
```

![](images/dice.gif)
:::
:::

::::

::: {.fragment fragment-index=3 data-id="box4" style="background: #ffc700; border: 4px solid #a68100; border-radius: 15px; width: 475px; height: 75px; padding: 0px 10px 20px 10px; position: absolute; top: 300px; right: -75px; font-size: 0.75em"}
Example:  Tossing a die to get a 3<br>
• In the short-run, might get very few or very many 3s<br>
• In the long-run 1/6 of the observations will be 3s
:::


## Myths about randomness and probability

### The myth of short-run regularity

:::: {.columns}

::: {.column width="30%"}
::: {style="color:#1b8883"}
[**Myth**]{.underline}: If I flip a coin 10 times I should get 5 heads.
:::

::: {style="color:#6e8e13"}
[**Reality**]{.underline}: Over a short number of flips, you  might get lots of heads or lots of tails.  Probabilities express expectations only over the long run.
:::

::: r-fit-text
There is a difference between [theoretical]{.underline} probability (long run) and [short-run experimental]{.underline} probability
:::

:::

::: {.column width="70%"}

```{r}
#| echo: false
#| eval: false
#| fig-width: 9
#| fig-height: 6
#| fig-align: center
#| 
zoom_die <- roll_3 + coord_cartesian(xlim = c(0, 100), ylim = c(0, 1) ,expand = TRUE, default = FALSE, clip = "on") + labs(title = "Zoomed in plot of the first 100 trials")

ggsave("lectures/images/dice.jpeg", zoom_die, height = 6, width = 9, units = "in")

```

![](images/dice.jpeg)

:::

::::


## Myths about randomness and probability

### The myth of short-run regularity

:::: {.columns}

::: {.column width="50%"}
::: {.fragment fragment-index=1 style="color:#1b8883"}
[**Myth**]{.underline}: I’ve flipped eight heads in a row, so I’m “due” for a tail.
:::

::: {.fragment fragment-index=2 style="color:#6e8e13"}
[**Reality**]{.underline}: Since each new flip (trial) is independent, the outcome of the next flip is not affected by what happened in past flips.
:::
:::

::: {.column width="50%"}
::: {.fragment fragment-index=1 style="font-size: 7em"}
🪙
:::

:::

::::

::: {.fragment fragment-index=3 style="font-size: 1.4em"}

> [**Independence**]{style="color:#e93cac"}<br>In probability, two events are independent if the incidence of<br>one event does not affect the probability of the other event.

:::

--- 

:::: {.columns}

::: {.column width="45%"}
<br>


### [Must think about probabilities over<br>the [**long**]{.underline} run]{style="font-size: 1em"}
Our inferences will be based on theoretical probabilities over a huge number of repeated trials

<br>

::: {.fragment fragment-index=2}
### Recall the key questions
What is the probability that the inference I draw from one sample is wrong (i.e. way off from the population characteristic of interest)?
:::
:::

::: {.column width="55%"}
::: {.fragment fragment-index=1}
<br>

### [Implications for [inferential]{style="color:#e93cac"} statistics]{style="font-size: 0.84em"}
* We typically use [one]{.underline} sample (very small number of trials) so may get a funky result.
* In the short-run (over a small number of trials) we might get sample results that do not represent the population very well.
* Won’t know how far off our one sample statistic is from the population parameter.
* Have to rely on probability [theory]{.underline} about how things would end up [over the long run]{.underline} (if we looked at a huge number of samples) to assess the probability of getting our one result.

:::
:::

::::


## Probability distribution

:::: {.columns}

::: {.column width="45%"}
::: r-fit-text
> [**Probability distribution**]{style="color:#e93cac"}<br>For a [random variable]{.underline},<br>this specifies its<br>possible values<br> and their probabilities. 
:::

<br>

::: {.fragment fragment-index=1}
A discrete random variable X has separate values (such as 1,2, 3…) as its possible outcomes
:::

<br>

::: {.fragment fragment-index=2}
Its probability distribution assigns a probability P(x) to each possible value of X
:::
:::

::: {.column width="55%"}
::: {.fragment fragment-index=4}
The sum of the probabilities for all the possible x values equals 1
:::

::: {.fragment fragment-index=2}
```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 6
#| fig-align: right
prob_dist <- tibble(x = 1:6, prob = rep(1/6, 6))

ggplot(prob_dist, aes(x = x, y = prob)) + 
  geom_col(fill = "#1b8883") + 
  ylim(0, 0.2) + 
  labs(x = "x = result of die role", y = "p(x)") + 
  theme_tufte(base_size = 18)
```

:::

::: {.fragment fragment-index=3 style="font-size: 0.95em"}
For each x, the probability P(x) falls between 0 and 1
:::
:::

::::

## Example: Probability distribution

#### What is the probability of a correct guess (choosing randomly) on a five-choice test question? 

:::: {.columns}

::: {.column width="60%"}
::: {.fragment fragment-index=1}
Probability distribution of guess outcomes

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 6
#| fig-align: left
prob_dist2 <- tibble(x = c("Correct", "Incorrect"), prob = c(0.2, 0.8))

ggplot(prob_dist2, aes(x = x, y = prob)) + 
  geom_col(fill = "#1b8883") + 
  ylim(0, 1) + 
  labs(x = "x = whether the guessed answer is correct", y = "p(x)") + 
  theme_tufte(base_size = 18)
```

:::
:::

::: {.column width="40%"}


::: {.fragment fragment-index=1 style="font-size: 1.5em"}
$$
P(C) = \frac{1}{5} = 0.2
$$

$$
P(I) = \frac{4}{5} = 0.8
$$

:::

<br>

::: {.fragment fragment-index=2 style="font-size: 1.25em"}
Note (again!):<br>[Probabilities = Proportions]{style="color: e93cac"}
:::
:::

::::

## Example: Probability distribution

#### Probability distributions are reflected in frequency distributions

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6
#| fig-align: left
prob_dist3 <- tibble(x = fct(c("No work", "Part time", "Full time", "More than\nfull time")), prob = c(25, 55, 17, 3))

work_example <- ggplot(prob_dist3, aes(x = x, y = prob, label = paste0(prob, "%"))) + 
  geom_col(fill = "#1b8883") + 
  geom_text(vjust = -0.5, size = 5) + 
  ylim(0, 60) + 
  labs(title = "Work status for sample of students", x = "", y = "") + 
  theme_tufte(base_size = 18) + 
  theme(axis.line.y = element_blank(), 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank())
ggsave(filename = "images/work_dist.png", plot = work_example, width = 10, height = 6, units = "in")
```

::: {.fragment fragment-index=1 data-id="box4" style="background: #ffc700; border: 4px solid #a68100; border-radius: 15px; width: 360px; height: 140px; padding: 0px 10px 20px 10px; position: absolute; top: 275px; right: 0px"}
As long as every possible outcome is included, we can think of this as a probability distribution
:::

## Example: Probability distribution

#### Probability distributions are reflected in frequency distributions

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6
#| fig-align: left
prob_dist4 <- tibble(x = fct(c("No work", "Part time", "Full time", "More than\nfull time")), prob = c(.25, .55, .17, .03))

ggplot(prob_dist4, aes(x = x, y = prob, label = prob)) + 
  geom_col(fill = c("#1b8883", "#1b8883", "#1b8883", "#754cbc")) + 
  geom_text(vjust = -0.5, size = 5) + 
  ylim(0, .60) + 
  labs(title = "Probability distribution of student work status", x = "", y = "p(x)") + 
  theme_tufte(base_size = 18) + 
  theme(axis.line.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        legend.position = "none")
```

::: {data-id="box4" style="background: #ffc700; border: 4px solid #a68100; border-radius: 15px; width: 360px; height: 140px; padding: 0px 10px 20px 10px; position: absolute; top: 275px; right: 0px"}
As long as every possible outcome is included, we can think of this as a probability distribution
:::

::: {.fragment fragment-index=2 data-id="box5" style="background: #c5b4e3; border: 4px solid #754cbc; border-radius: 15px; width: 210px; height: 125px; padding: 0px 0px 20px 10px; position: absolute; bottom: 0px; right: -85px; font-size: 0.75em"}
For example: The<br>probability of randomly selecting a student who works more than full time is .03
:::

## Probability rules

<br>

:::: {.columns}

::: {.column width="50%"}
### **Rule 1**
#### The probability $P(A)$ of any event A<br>satisfies **$0 \leq P(A) \leq 1$**

<br>

[The probability of any single event, is always between 0 and 1.0 ]{style="font-size: 1.25em"}

* [An event, or outcome, with probability 0 never happens]{style="color: #e93cac"}
* [An event, or outcome, with probability 1 is a certainty]{style="color: #e93cac"}

:::

::: {.column width="50%"}
::: {.fragment fragment-index=1}
### **Rule 2**
#### If $S$ is the sample space in a<br>probability model, then **$P(S) = 1$**

<br>

[Add up the probabilities of all these possible outcomes and the result will be 1.0]{style="font-size: 1.25em"}

* [Sample space is just the list of all possible outcomes]{style="color: #e93cac"}
* [A list of all the possibilities equals 100% of possibilities and one of them has to happen as a result of the trial]{style="color: #e93cac"}

:::
:::

::::

## Probability rules

:::: {.columns}

::: {.column width="45%"}

<br>

### **Rule 3**: Addition Rule
#### The probability of [any]{.underline} of several different mutually exclusive outcomes is equal to the sum of their separate probabilities
<br>
[i.e., the probability of [**either**]{style="color: #e93cac"} outcome A [**or**]{style="color: #e93cac"} outcome B on [one single trial]{.underline}:]{style="font-size: 1.25em"}

<br>

[$$
P(\text{A or B}) = P(A) + P(B)
$$]{style="font-size: 1.25em"}

:::

::: {.column width="55%"}
::: {.fragment fragment-index=2}
#### Examples

* Probability of rolling a *5* or a *6* with a fair six-sided die?

$$
P(\text{5 or 6}) = \frac{1}{6} + \frac{1}{6} = 0.167 + 0.167 = 0.334
$$

* Probability of rolling any value below *4* with a fair six-sided die?

$$
P(\text{1 or 2 or 3}) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = 0.167 + 0.167 + 0.167 = 0.50
$$
:::

::: {.fragment fragment-index=3 data-id="box3" style="background: #aadb1e; border: 4px solid #6e8e13; border-radius: 15px; width: 575px; height: 100px; padding: 0px 10px 20px 10px; position: absolute; bottom: 25px; right: -50px"}
The probability of [any]{.underline} of multiple outcomes happening is always [larger]{.underline} than the probability of any one of the outcomes alone.
:::

:::

::::

## Probability rules

:::: {.columns}

::: {.column width="60%"}
::: {.fragment fragment-index=2}
#### Examples

* Probability of rolling a *5* and a *6* on consecutive roles of a fair six-sided die?

$$
P(\text{5 and 6}) = \frac{1}{6} \times \frac{1}{6} = (0.167)(0.167) = 0.028
$$

* Probability of flipping 3 heads in a row?

$$
P(\text{H, H, H}) = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} = (0.5)(0.5)(0.5)= 0.125
$$
:::


::: {.fragment fragment-index=3 data-id="box3" style="background: #aadb1e; border: 4px solid #6e8e13; border-radius: 15px; width: 475px; height: 140px; padding: 0px 10px 20px 10px; position: absolute; bottom: 25px; left: 0px"}
The probability of a combination of [multiple]{.underline} outcomes happening is always [smaller]{.underline} than the probability of one of the outcomes happening independently.

:::
:::

::: {.column width="40%"}

### **Rule 4**:<br>Multiplication Rule
#### The probability of a combination of [independent]{.underline} outcomes is equal to the product of their separate probabilities
<br>
[i.e., the probability of outcome A [**and**]{style="color: #e93cac"} outcome B on [two independent trials]{.underline}:]{style="font-size: 1.25em"}

<br>

[$$
P(\text{A and B}) = P(A) \times P(B)
$$]{style="font-size: 1.25em"}

:::

::::

---

### [[Examples]{style="color: #e93cac"}: What is the probability of randomly selecting…]{style="font-size: 0.95em"}

:::: {.columns}

::: {.column width="33.33%"}
#### A single student who does not work?
:::

::: {.column width="33.33%"}
#### Two consecutive students who do not work?
:::

::: {.column width="33.33%"}
#### A single student who works at least full time?
:::

::::

![](images/work_dist.png){.absolute bottom=-50 left=100 width="800"}

---

### [[Examples]{style="color: #e93cac"}: What is the probability of randomly selecting…]{style="font-size: 0.95em"}

:::: {.columns}

::: {.column width="33.33%"}
#### A single student who does not work?

$P(nw) = 0.25$
:::

::: {.column width="33.33%"}



:::

::: {.column width="33.33%"}



:::

::::

![](images/work_dist.png){.absolute bottom=-50 left=100 width="800"}

---

### [[Examples]{style="color: #e93cac"}: What is the probability of randomly selecting…]{style="font-size: 0.95em"}

:::: {.columns}

::: {.column width="33.33%"}



:::

::: {.column width="33.33%"}
#### Two consecutive students who do not work?

$P(\text{nw, nw}) = (0.25)(0.25) = 0.0625$
:::

::: {.column width="33.33%"}



:::

::::

![](images/work_dist.png){.absolute bottom=-50 left=100 width="800"}

---

### [[Examples]{style="color: #e93cac"}: What is the probability of randomly selecting…]{style="font-size: 0.95em"}

:::: {.columns}

::: {.column width="33.33%"}



:::

::: {.column width="33.33%"}



:::

::: {.column width="33.33%"}
#### A single student who works at least full time?

$P(\text{ft or mft}) = 0.17 + 0.03 = 0.20$
:::

::::

![](images/work_dist.png){.absolute bottom=-50 left=100 width="800"}

---

### Examples: [Probability distribution]{style="color: #e93cac"}

<br>

#### Probability distributions are reflected in frequency distribution

![](images/work_dist.png){.absolute bottom=-50 left=100 width="800"}

::: {.fragment fragment-index=1 data-id="box1" style="width: 500px; height: 250px; margin: 10px; position: absolute; top: 175px; right: -75px"}
::: r-fit-text
>[Discrete probability distribution]{style="color: #e93cac"}<br>Shows the probability of<br>a set of distinct,<br>separable outcomes.
:::
:::

---

### Examples: [Continuous probability distribution]{style="color: #e93cac"}

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6
#| fig-align: left

prob_dist5 <- tibble(hours = 0:45, perc = c(1, 1.25, 1.5, 1.75, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.6, 3.7, 3.9, 4, 4.25, 4.5, 4.75, 4.75, 4.5, 4, 3.75, 3.5, 3.25, 3, 2.75, 2.5, 2.25, 2, 1.75, 1.5, 1.25, 1.1, 1, 0.75, 0.5, 0.25, 0.5, 0.75, 1, 1.25, 1, 0.75, 0.5, 0.25, 0.25, 0.25))

ggplot(prob_dist5, aes(x = hours, y = perc)) + 
  geom_histogram(color = "black", fill = "#1b8883", stat = "identity") + 
  #geom_density(color = "red", stat = "identity")
  ylim(0, 5) + 
  labs(title = "Study time for sample of students", x = "Hours studied per week", y = "% of students") + 
  theme_tufte(base_size = 18) 
```

::: {.fragment fragment-index=1 data-id="box1" style="width: 500px; height: 100px; margin: 10px; position: absolute; top: 50px; right: -75px"}
::: r-fit-text
>[Probability density function]{style="color: #e93cac"}<br>Shows the probability of a<br>set of continuous outcomes.
:::
:::

::: {.fragment fragment-index=1 data-id="box1" style="width: 500px; height: 50px; margin: 10px; position: absolute; top: 250px; right: -75px"}
Can use a histogram to create a continuous probability distribution
:::

::: {.fragment fragment-index=2 data-id="box2" style="background: #c5b4e3; border: 4px solid #754cbc; border-radius: 15px; width: 500px; height: 110px; padding: 0px 0px 10px 10px; position: absolute; top: 350px; right: -75px"}
**Note**: Continuous random variables have an infinite continuum of possible values in an interval (e.g., time, age, height, weight...)
:::

::: {.fragment fragment-index=2 data-id="box4" style="background: #ffc700; border: 4px solid #a68100; border-radius: 15px; width: 450px; height: 60px; padding: 0px 10px 20px 10px; position: absolute; top: 500px; right: -75px"}
Contrast to a discrete random variable (limited number of possible values)
:::

---

### Examples: [Continuous probability distribution]{style="color: #e93cac"}

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6
#| fig-align: left

prob_dist5 <- tibble(hours = 0:45, perc = c(1, 1.25, 1.5, 1.75, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.6, 3.7, 3.9, 4, 4.25, 4.5, 4.75, 4.75, 4.5, 4, 3.75, 3.5, 3.25, 3, 2.75, 2.5, 2.25, 2, 1.75, 1.5, 1.25, 1.1, 1, 0.75, 0.5, 0.25, 0.5, 0.75, 1, 1.25, 1, 0.75, 0.5, 0.25, 0.25, 0.25))

ggplot(prob_dist5, aes(x = hours, y = perc)) + 
  geom_histogram(color = "black", fill = "#1b8883", stat = "identity") + 
  #geom_density(color = "red", stat = "identity")
  ylim(0, 5) + 
  labs(title = "Study time for sample of students", x = "Hours studied per week", y = "% of students") + 
  theme_tufte(base_size = 18) 
```


## Probability density function

# Break! {.section-title background-color="#2ad2c9"}

# Normal distributions {.section-title background-color="#c5b4e3"}

## Normal distribution

## Consistent areas under the normal curve

## Standard normal distribution

## Normal distribution and standardized z-scores

## Steps for finding area under the normal curve

*Note: these steps can happen out of order!*

1. Draw the picture, roughly showing the area under the curve that you are looking for (highly recommended)
2. Convert value(s) of interest into z-score(s)
3. Use standard normal table (linked in the back of the book) to find proportion of cases between the mean and the z-score
4. Subtract or add areas under the curve to get the total proportion you are looking for (see the picture from step 1)
5. Convert to percentages or probabilities as required by the problem

## Step 1

## Step 2

## Step 3

## Step 4

## Step 5

## Examples

# Homework{.section-title background-color="#e8e3d3"}

## {data-menu-title="Homework 4" background-iframe="https://vsass.github.io/SOC221/homework/homework4.html" background-interactive=TRUE}